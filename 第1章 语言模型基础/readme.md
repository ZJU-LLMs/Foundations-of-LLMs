# 语言模型基础论文列表

- [语言模型基础](#语言模型基础)
  - [基于统计方法的语言模型](#基于统计方法的语言模型)
  - [基于 RNN 的语言模型](#基于-rnn-的语言模型)
  - [基于 Transformer 的语言模型](#基于-transformer-的语言模型)
  - [语言模型的采样方法](#语言模型的采样方法)
  - [语言模型的评测](#语言模型的评测)


## <img src="../figure/star.svg" width="25" height="25" />基于统计方法的语言模型

1. **Foundations of statistical natural language processing.** `BOOK`  
    *Chris Manning, Hinrich Sch{\"{u}}tze* [[PDF](https://nlp.stanford.edu/fsnlp/)], 1999

2. **Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition.Third Edition.** `BOOK`  
    *Daniel Jurafsky, James H. Martin* [[PDF](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf)], 2023


## <img src="../figure/star.svg" width="25" height="25" />基于 RNN 的语言模型

1. **A learning algorithm for continually running fully recurrent neural networks.** `Neural computation`  
    *RJ Williams, D Zipser.* [[PDF](https://gwern.net/doc/ai/nn/rnn/1989-williams-2.pdf)], 1989
  
2. **Long Short-Term Memory.** `Neural Computing`  
    *Sepp Hochreiter, J{\"{u}}rgen Schmidhuber* [[PDF](https://deeplearning.cs.cmu.edu/F23/document/readings/LSTM.pdf)], 1997

3. **On the difficulty of training Recurrent Neural Networks.** `ICML`  
    *Razvan Pascanu, Tomas Mikolov, Yoshua Bengio.* [[PDF](https://arxiv.org/abs/1211.5063)], 2012

4. **Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.** `arXiv`  
    *Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio* [[PDF](https://arxiv.org/abs/1412.3555)], 2014 

5. **Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.** `NeurIPS`  
    *Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer* [[PDF](https://arxiv.org/abs/1506.03099)], 2015    





## <img src="../figure/star.svg" width="25" height="25" />基于 Transformer 的语言模型

1. **Layer Normalization.** `arXiv`  
    *Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton* [[PDF](https://arxiv.org/abs/1607.06450)], 2016

2. **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.** `JMLR`  
*Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu.* [[PDF](https://arxiv.org/abs/1910.10683)], 2019

3. **Transformer Feed-Forward Layers Are Key-Value Memories.** `EMNLP`  
    *Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy* [[PDF](https://arxiv.org/abs/2012.14913)], 2021

4. **ResiDual: Transformer with Dual Residual Connections.** `arXiv`  
    *Shufang Xie, Huishuai Zhang, Junliang Guo, Xu Tan, Jiang Bian, Hany Hassan Awadalla, Arul Menezes, Tao Qin, Rui Yan.* [[PDF](https://arxiv.org/abs/2304.14802)], 2023




## <img src="../figure/star.svg" width="25" height="25" />语言模型的采样方法

1. **Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models.** `AAAI`  
    *Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R. Selvaraju, Qing Sun, Stefan Lee, David Crandall, Dhruv Batra.* [[PDF](https://arxiv.org/abs/1610.02424)], 2018

2. **The Curious Case of Neural Text Degeneration.** `ICLR`  
    *Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi* [[PDF](https://arxiv.org/abs/1904.09751)], 2020




## <img src="../figure/star.svg" width="25" height="25" />语言模型的评测


1. **Perplexity—a Measure of the Difficulty of Speech Recognition Tasks.** `JASA`  
    *F. Jelinek, R. L. Mercer, L. R. Bahl, J. K. Baker* [[PDF](https://pubs.aip.org/asa/jasa/article/62/S1/S63/642598/Perplexity-a-measure-of-the-difficulty-of-speech)], 1997

2. **ROUGE: A Package for Automatic Evaluation of Summaries.** `ACL`  
    *Chin-Yew Lin* [[PDF](https://aclanthology.org/W04-1013/)], 2004

3. **BLEU might be Guilty but References are not Innocent.** `EMNLP`  
    *Markus Freitag, David Grangier, Isaac Caswell* [[PDF](https://arxiv.org/abs/2004.06063)], 2020

4. **BERTScore: Evaluating Text Generation with BERT.** `ICLR`  
    * Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, Yoav Artzi.* [[PDF](https://arxiv.org/abs/1904.09675)], 2020

5. **Leveraging Large Language Models for NLG Evaluation: Advances and Challenges.** `arXiv`  
    *Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, Shuai Ma* [[PDF](https://arxiv.org/abs/2401.07103)], 2024

6. **G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment.** `EMNLP`  
    *Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu* [[PDF](https://arxiv.org/abs/2303.16634)], 2023

7. **INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback.** `EMNLP`  
    * Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, Lei Li.* [[PDF](https://aclanthology.org/2023.emnlp-main.365/)], 2023

































  





  




